# TinyGraph Configuration File
# This file defines the distributed graph database cluster configuration

query_manager:
  host: "localhost"
  port: 9090

# Shard configuration - each shard stores a partition of the graph
# Each shard has multiple replicas that use Raft for consensus
shards:
  - id: 0
    replicas:
      # Primary replica (bootstrap the Raft cluster)
      - id: 0
        rpc_host: "localhost"
        rpc_port: 9091
        raft_host: "localhost"
        raft_port: 10091
        raft_dir: "./raft-data/shard-0-replica-0"
        bootstrap: true
      
      # Secondary replicas (join the Raft cluster)
      - id: 1
        rpc_host: "localhost"
        rpc_port: 9092
        raft_host: "localhost"
        raft_port: 10092
        raft_dir: "./raft-data/shard-0-replica-1"
        bootstrap: false
      
      - id: 2
        rpc_host: "localhost"
        rpc_port: 9093
        raft_host: "localhost"
        raft_port: 10093
        raft_dir: "./raft-data/shard-0-replica-2"
        bootstrap: false
    
    # TODO: Add more shards here

# Partitioning strategy defines how vertices/edges are distributed across shards
partitioning:
  algorithm: "hash"  # Options: "hash", ...

# Replication strategy defines how data is replicated for fault tolerance
replication:
  strategy: "raft"  # Options: "none", "raft"
  replication_factor: 3  # Number of replicas per shard

